{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des outils / jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"data/data-cleaned-feature-engineering.csv\",\n",
    "    sep=\",\",\n",
    "    index_col=\"ID\",\n",
    "    parse_dates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transforme = pd.read_csv(\n",
    "    \"data/data-transformed.csv\",\n",
    "    sep=\",\",\n",
    "    index_col=\"ID\",\n",
    "    parse_dates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_numeriques = [\n",
    "    \"Year_Birth\",\n",
    "    \"Income\",\n",
    "    \"Recency\",\n",
    "    \"MntWines\",\n",
    "    \"MntFruits\",\n",
    "    \"MntMeatProducts\",\n",
    "    \"MntFishProducts\",\n",
    "    \"MntSweetProducts\",\n",
    "    \"MntGoldProds\",\n",
    "    \"NumDealsPurchases\",\n",
    "    \"NumWebPurchases\",\n",
    "    \"NumCatalogPurchases\",\n",
    "    \"NumStorePurchases\",\n",
    "    \"NumWebVisitsMonth\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_categoriques = [\n",
    "    \"Education\",\n",
    "    \"Marital_Status\",\n",
    "    \"Kidhome\",\n",
    "    \"Teenhome\",\n",
    "    \"Complain\",\n",
    "    \"AcceptedCmp1\",\n",
    "    \"AcceptedCmp2\",\n",
    "    \"AcceptedCmp3\",\n",
    "    \"AcceptedCmp4\",\n",
    "    \"AcceptedCmp5\",\n",
    "    \"Response\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df_transforme.drop(columns=[\"Response\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[[\"Response\"]].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, labels=log_reg.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred, labels=log_reg.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=log_reg.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: commenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle d'ajustement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de l'objet PLSRegression avec 2 composantes PLS\n",
    "pls = PLSRegression(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apprentissage du modèle sur les données\n",
    "pls.fit(df_transforme[var_numeriques], df_transforme[\"NumStorePurchases\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction de la variable cible sur de nouvelles données\n",
    "y_pred = pls.predict(df_transforme[var_numeriques])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation de la performance du modèle\n",
    "r2 = pls.score(\n",
    "    df_transforme[var_numeriques], df_transforme[\"NumStorePurchases\"].astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle de prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_transforme[var_numeriques].drop(columns=[\"NumStorePurchases\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[[\"NumStorePurchases\"]].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de l'objet PLSRegression avec 2 composantes PLS\n",
    "pls = PLSRegression(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apprentissage du modèle sur les données\n",
    "pls.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction de la variable cible sur de nouvelles données\n",
    "y_pred = pls.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation de la performance du modèle\n",
    "pls.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE = {mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"RMSE = {mean_squared_error(y_test, y_pred, squared=False)}\")\n",
    "print(f\"MAE = {mean_absolute_error(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression linéaire simple"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle simple : une variable à expliquer $Y$ et une seule variable explicative $X$.\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$$\n",
    "\n",
    "Hypothèses à vérifier pour la régression linéaire simple :\n",
    "\n",
    "1) il existe une corrélation linéaire entre $X$ et $Y$\n",
    "\n",
    "1) la distribution de l’erreur $\\epsilon$ est indépendante de la variable X (exogénéité)\n",
    "\n",
    "2) l'erreur suit une loi normale centrée i.e. $E(ε_i) = 0$\n",
    "\n",
    "3) l’erreur est de variance constante (homoscédasticité)\n",
    "i.e $Var(ε_i) = \\sigma^2$, une constante\n",
    "\n",
    "4) les erreurs sont indépendantes (absence d'autocorrélation)\n",
    "i.e. $Cov(εi, εj) = 0$, pour tout $i, j \\in N \\times N, i \\neq j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df_transforme[\"Income\"])\n",
    "Y = np.array(df_transforme[\"NumStorePurchases\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 1 : corrélation linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Le coefficient de corrélation linéaire entre X et Y vaut\",\n",
    "    pearsonr(X, Y)[0],\n",
    "    \"la pvalue associée vaut\",\n",
    "    pearsonr(X, Y)[1],\n",
    "    \"il existe donc bien une relation linéaire entre X et Y.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le reste des hypothèses à tester requiert d'effectuer la régression linéaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X_train.reshape(-1, 1), Y_train)\n",
    "Y_train_hat = model.predict(X_train.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, Y_train, color=\"black\")\n",
    "plt.plot(X_train, Y_train_hat, color=\"red\")\n",
    "plt.title(\"Régression linéaire du nombre d'achats en magasin en fonction du revenu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.intercept_, model.coef_, model.score(X_train.reshape(-1, 1), Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation de régression :\n",
    "\n",
    "$$y_i = 0.46 + 2.21 * x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 2 : exogénéité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = Y_train - Y_train_hat\n",
    "\n",
    "plt.scatter(X_train, residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Le coefficient de corrélation entre X et les residus vaut\",\n",
    "    pearsonr(X_train, residuals)[0],\n",
    "    \". On a bien indépendance et donc exogénéité.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 3 : l'erreur suit une loi normale centrée i.e. E(ε_i) = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(residuals, density=True)\n",
    "\n",
    "x = np.linspace(-7, 7, 100)\n",
    "plt.plot(x, stats.norm.pdf(x, 0, 1))\n",
    "plt.title(\"Histogramme des résidus en superposition avec la densité de la loi normale\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La moyenne des résidus vaut\", statistics.mean(residuals))\n",
    "print(\"Mais les résidus ne suivent pas une loi normale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(residuals, line=\"45\")\n",
    "print(\"On constate sur le qqplot que les points ne suivent pas la droite x = y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Un test de shapiro, pour tester l'hypothèse de normalité, nous donne une pvalue de\",\n",
    "    stats.shapiro(residuals)[1],\n",
    "    \". On rejette l'hypothèse nulle et on conclut que les résidus ne suivent pas une loi normale.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 4 : homoscédasticité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(slope, intercept):\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, \"-\", color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(residuals, \"bo\")\n",
    "plt.title(\"Nuage de points des résidus\")\n",
    "abline(0, 7)\n",
    "abline(0, -7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.stats.api as sms\n",
    "from statsmodels.compat import lzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = sm.OLS(Y_train, sm.add_constant(X_train)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_names = [\"F statistic\", \"p-value\"]\n",
    "gq_test = sms.het_goldfeldquandt(fit.resid, fit.model.exog)\n",
    "lzip(res_names, gq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_test = sms.het_breuschpagan(fit.resid, fit.model.exog)\n",
    "lzip(res_names, bp_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le test de Goldfeld-Quandt ne nous donne pas d'hétéroscédasticité, contraiement au test de Breusch-Pagan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 5 : absence d'autocorrélation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.regressionplots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.tsa.plot_acf(residuals)\n",
    "print(\"On remarque une absence d'autocorrélation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance de Cook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence = fit.get_influence()\n",
    "cooks = influence.cooks_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, cooks[0])\n",
    "plt.xlabel(\"Revenus\")\n",
    "plt.ylabel(\"Distances de Cook\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooks_indexes = [i for i, x in enumerate(cooks[0] > 0.005) if x]\n",
    "print(cooks_indexes)\n",
    "# affiche les indexes des individus dont les distances de cook dépassent une certaine valeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_plot(fit)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualité d'ajustement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Le R² du modèle vaut {model.score(X_train.reshape(-1, 1), Y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE = {mean_squared_error(Y_train, Y_train_hat)}\")\n",
    "print(f\"RMSE = {mean_squared_error(Y_train, Y_train_hat, squared=False)}\")\n",
    "print(f\"MAE = {mean_absolute_error(Y_train, Y_train_hat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualité de prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"R² du modèle sur les données d'entraînement = {model.score(X_train.reshape(-1, 1), Y_train)}\"\n",
    ")\n",
    "print(\n",
    "    f\"R² du modèle sur les données de test = {model.score(X_test.reshape(-1, 1), Y_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_hat = model.predict(X_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test, Y_test, color=\"black\")\n",
    "plt.plot(X_test, Y_test_hat, color=\"red\")\n",
    "plt.title(\"Nombre d'achats en magasin en fonction du revenu : données test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE = {mean_squared_error(Y_test, Y_test_hat)}\")\n",
    "print(f\"RMSE = {mean_squared_error(Y_test, Y_test_hat, squared=False)}\")\n",
    "print(f\"MAE = {mean_absolute_error(Y_test, Y_test_hat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression linéaire multiple"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle multiple : une variable à expliquer $Y$ et $P$ variables explicatives $X_i$\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 X_i(1) + \\beta_2 X_i(2) + \\beta_3 X_i(3) + ... + \\beta_P X_i(P) + \\epsilon_i$$\n",
    "\n",
    "Hypothèses à vérifier pour la régression linéaire multiple :\n",
    "\n",
    "1) il existe une corrélation linéaire entre $X_i$ et $Y$\n",
    "\n",
    "2) $Cov(X_i, ε_j) = 0$, pour tout $i, j \\in N \\times N, i \\neq j$ (exogénéité) et pour chaque variable explicative $X_p$\n",
    "\n",
    "3) l'erreur suit une loi normale centrée i.e. $E(ε_i) = 0$\n",
    "\n",
    "4) l’erreur est de variance constante (homoscédasticité)\n",
    "i.e $Var(ε_i) = \\sigma^2$, une constante\n",
    "\n",
    "5) les erreurs sont indépendantes (absence d'autocorrélation)\n",
    "i.e. $Cov(εi, εj) = 0$, pour tout $i, j \\in N \\times N, i \\neq j$\n",
    "\n",
    "6) absence de colinéarité entre les variables explicatives,\n",
    "i.e. $X^t * X$ est régulière, $det(X^t * X) \\neq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les besoins de la régression linéaire, nous créons des variables muettes (dummy variables) pour inclure les variables catégoriques `Education` et `Marital_status` à la régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = df_transforme.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = pd.get_dummies(df_reg, columns=[\"Education\", \"Marital_Status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cette régression, on suppose que l'entreprise veut profiler au mieux les clients qui achètent dans le magasin. C'est pourquoi la variable d'intérêt pour la régression sera le `nombre d'achats en magasin`.\n",
    "\n",
    "Nos variables explicatives sont des variables numériques et des variables catégoriques transformées en variables muettes.\n",
    "Nous commençons par un grand nombre de variables explicatives puis nous en éliminerons progressivement pendant la vérifications des hypothèses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_numeriques_reg = [\n",
    "    \"Income\",\n",
    "    \"Year_Birth\",\n",
    "    \"Recency\",\n",
    "    \"NbAcceptedCampaigns\",\n",
    "    \"NumWebPurchases\",\n",
    "    \"NumDealsPurchases\",\n",
    "    \"NumWebVisitsMonth\",\n",
    "    \"NumCatalogPurchases\",\n",
    "    \"MntWines\",\n",
    "    \"MntFruits\",\n",
    "    \"MntMeatProducts\",\n",
    "    \"MntFishProducts\",\n",
    "    \"MntSweetProducts\",\n",
    "    \"MntGoldProds\",\n",
    "]\n",
    "\n",
    "var_categoriques_reg = [\n",
    "    \"NbChildren\",\n",
    "    \"Education_2n Cycle\",\n",
    "    \"Education_Basic\",\n",
    "    \"Education_Graduation\",\n",
    "    \"Education_Master\",\n",
    "    \"Education_PhD\",\n",
    "    \"Marital_Status_Divorced\",\n",
    "    \"Marital_Status_Married\",\n",
    "    \"Marital_Status_Single\",\n",
    "    \"Marital_Status_Together\",\n",
    "    \"Marital_Status_Widow\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reg[var_categoriques_reg + var_numeriques_reg]\n",
    "Y = df_reg[\"NumStorePurchases\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Nous avons\",\n",
    "    X.shape[1],\n",
    "    \"variables explicatives au début de la régression dont\",\n",
    "    X[var_numeriques_reg].shape[1],\n",
    "    \"variables numériques.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 1 : lien linéaire entre Y et les variables explicatives numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_transforme, x_vars=var_numeriques_reg[0:4], y_vars=\"NumStorePurchases\")\n",
    "sns.pairplot(df_transforme, x_vars=var_numeriques_reg[4:8], y_vars=\"NumStorePurchases\")\n",
    "sns.pairplot(df_transforme, x_vars=var_numeriques_reg[8:12], y_vars=\"NumStorePurchases\")\n",
    "sns.pairplot(df_transforme, x_vars=var_numeriques_reg[12:], y_vars=\"NumStorePurchases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X[var_numeriques_reg].columns:\n",
    "    corr = pearsonr(X[x], Y)\n",
    "    print(\n",
    "        \"Le coefficient de corrélation linéaire entre Y et\",\n",
    "        x,\n",
    "        \"vaut\",\n",
    "        corr[0],\n",
    "        \"la pvalue vaut\",\n",
    "        corr[1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: faire un joli affichage avec SNS heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On décide déjà d'écarter la variable `Recency` de la régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X[\"Recency\"]\n",
    "var_numeriques_reg.remove(\"Recency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réalisation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multiple = LinearRegression().fit(X_train, Y_train)\n",
    "Y_train_hat = model_multiple.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_train, Y_train_hat, color=\"black\")\n",
    "plt.title(\"Valeurs prédites contres vraies valeurs (donnéees d'entraînement)\")\n",
    "abline(1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 2 : exogénéité sur les variables numériques\n",
    "(Cela n'a pas de sens de tester l'exogénéité sur les variables catégoriques car la notion de corrélation ne fonctionne pas avec celles-ci.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = Y_train - Y_train_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X_train[var_numeriques_reg].columns:\n",
    "    corr = pearsonr(X_train[x], residuals)\n",
    "    print(\n",
    "        \"Le coefficient de corrélation entre\",\n",
    "        x,\n",
    "        \"et les residus vaut\",\n",
    "        corr[0],\n",
    "        \"et la pvalue associée vaut\",\n",
    "        corr[1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il n'y a pas d'endogénéité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 3 : l'erreur suit une loi normale centrée i.e. E(ε_i) = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(residuals, density=True)\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "plt.plot(x, stats.norm.pdf(x, 0, 1))\n",
    "plt.title(\"Histogramme des résidus en superposition avec la densité de la loi normale\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La moyenne des résidus vaut\", statistics.mean(residuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(residuals, line=\"45\")\n",
    "print(\"On constate sur le qqplot que les points ne suivent pas la droite x = y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Un test de shapiro, pour tester l'hypothèse de normalité, nous donne une pvalue de\",\n",
    "    stats.shapiro(residuals)[1],\n",
    "    \". On rejette l'hypothèse nulle et on conclut que les résidus ne suivent pas une loi normale.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 4 : homoscédasticité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(residuals, \"bo\")\n",
    "plt.title(\"Nuage de points des résidus\")\n",
    "abline(0, 6.5)\n",
    "abline(0, -6.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = sm.OLS(Y_train, sm.add_constant(X_train)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_names = [\"F statistic\", \"p-value\"]\n",
    "gq_test = sms.het_goldfeldquandt(fit.resid, fit.model.exog)\n",
    "lzip(res_names, gq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_test = sms.het_breuschpagan(fit.resid, fit.model.exog)\n",
    "lzip(res_names, bp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_test = sms.het_white(fit.resid, fit.model.exog)\n",
    "lzip(res_names, white_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le test de Goldfeld-Quandt, qui vérifie la constance de la variance entre deux échantillons, ne nous donne pas d'hétéroscédasticité, contrairement aux tests de White et de Breusch-Pagan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 5 : absence d'autocorrélation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.tsa.plot_acf(residuals)\n",
    "print(\"On remarque une absence d'autocorrélation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduction du nombre de variables explicatives par le critère AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aic(X, Y, regressor=\"\"):  # entrer X le dataframe des variables explicatives,\n",
    "    # Y la variable expliquée, et le regresseur utilisé\n",
    "    aic_list = []\n",
    "\n",
    "    for col in X.columns:\n",
    "        X_temp = X.drop(col, axis=1)\n",
    "\n",
    "        if regressor == \"linearOLS\":\n",
    "            model_temp = sm.OLS(Y, sm.add_constant(X_temp)).fit()\n",
    "        elif regressor == \"GLMpoisson\":\n",
    "            model_temp = sm.GLM(\n",
    "                Y, sm.add_constant(X_temp), family=sm.families.Poisson()\n",
    "            ).fit()\n",
    "\n",
    "        else:\n",
    "            return \"Entrer un régresseur valide\"\n",
    "\n",
    "        aic_list.append([col, model_temp.aic])\n",
    "\n",
    "    return aic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise(X, Y, regressor=\"\"):  # entrer X le dataframe des variables explicatives,\n",
    "    # Y la variable expliquée, et le regresseur utilisé\n",
    "\n",
    "    if regressor == \"linearOLS\":\n",
    "        current_aic = (\n",
    "            sm.OLS(Y, sm.add_constant(X)).fit().aic\n",
    "        )  # aic avec les variables actuelles\n",
    "    elif regressor == \"GLMpoisson\":\n",
    "        current_aic = (\n",
    "            sm.GLM(Y, sm.add_constant(X), family=sm.families.Poisson()).fit().aic\n",
    "        )\n",
    "    else:\n",
    "        return \"Entrer un régresseur valide\"\n",
    "\n",
    "    aic_list = aic(\n",
    "        X, Y, regressor\n",
    "    )  # liste des aic par supression des variables une a une\n",
    "\n",
    "    my_bool = 0  # pour indiquer si aucune variable n'est a enlever\n",
    "    for i in range(\n",
    "        len(aic_list)\n",
    "    ):  # si l'aic diminue pour une variable supprimee, on enleve cette variable du df X\n",
    "\n",
    "        if aic_list[i][1] < current_aic:\n",
    "            del X[aic_list[i][0]]  # a l'interieur des crochets c'est une string\n",
    "            my_bool = 1\n",
    "\n",
    "    if my_bool == 0:\n",
    "        return \"L'algorithme stepwise est terminé.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avant l'AIC, le nombre de variables explicatives est :\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    if (\n",
    "        stepwise(X_train, Y_train, regressor=\"linearOLS\")\n",
    "        == \"L'algorithme stepwise est terminé.\"\n",
    "    ):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise(X_train, Y_train, regressor=\"linearOLS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Après l'AIC, le nombre de variables explicatives est :\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on refait les modèles après sélection de variables\n",
    "\n",
    "X_test = X_test[\n",
    "    X_train.columns\n",
    "]  # homogénéisation des données de test et d'entraînement\n",
    "\n",
    "fit = sm.OLS(Y_train, sm.add_constant(X_train)).fit()\n",
    "\n",
    "model_multiple = LinearRegression().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance de Cook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence = fit.get_influence()\n",
    "cooks = influence.cooks_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train.index, cooks[0])\n",
    "plt.xlabel(\"Index des individus\")\n",
    "plt.ylabel(\"Distances de Cook\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooks_indexes = [i for i, x in enumerate(cooks[0] > 0.010) if x]\n",
    "# détermine l'index dans la liste des individus dont la distance de cook dépasse 0.010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cooks_indexes:\n",
    "    print(X_train.iloc[[i]].index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: supprimer les individus aux distances trop grandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_plot(fit)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les distances de Cook ont été largement réduites par la sélection de variables au critère AIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualité d'ajustement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Le R² du modèle vaut {model_multiple.score(X_train, Y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_hat = model_multiple.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE = {mean_squared_error(Y_train, Y_train_hat)}\")\n",
    "print(f\"RMSE = {mean_squared_error(Y_train, Y_train_hat, squared=False)}\")\n",
    "print(f\"MAE = {mean_absolute_error(Y_train, Y_train_hat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualité de prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"R² du modèle sur les données d'entraînement = {model_multiple.score(X_train, Y_train)}\"\n",
    ")\n",
    "print(f\"R² du modèle sur les données de test = {model_multiple.score(X_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_hat = model_multiple.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_test, Y_test_hat, color=\"black\")\n",
    "abline(1, 0)\n",
    "plt.title(\"Valeurs prédites contre les vraies valeurs : données de test du modèle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE = {mean_squared_error(Y_test, Y_test_hat)}\")\n",
    "print(f\"RMSE = {mean_squared_error(Y_test, Y_test_hat, squared=False)}\")\n",
    "print(f\"MAE = {mean_absolute_error(Y_test, Y_test_hat)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression par modèle linéaire généralisé (GLM) : régression de Poisson\n",
    "\n",
    "- Notons $Y$ notre variable d'intérêt et $X_1, ..., X_P$ nos variables explicatives.  \n",
    "- On suppose que Y suit une loi de Poisson de paramètre $\\lambda \\in \\mathbb{R+*}$\n",
    "\n",
    "\n",
    "- On cherche à déterminer les coefficients de l'équation :\n",
    "\n",
    "$\\log(\\mathrm{E}(Y|X)) = \\log( \\lambda ) = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_P X_P $\n",
    "\n",
    "Ce qui revient à déterminer les coefficients de l'équation :\n",
    "\n",
    "$\\lambda = e^{\\alpha + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_P X_P}$\n",
    "\n",
    "L'objectif de la régression est d'estimer $\\alpha, \\beta_1, ..., \\beta_P$.  \n",
    "Une fois ces coefficients estimés, on peut déterminer, pour un nouveau vecteur $X$,  \n",
    "$E(Y|X) = e^{\\alpha + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_P X_P}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèses à vérifier : \n",
    "\n",
    "<br>\n",
    "- Avant de choisir la régression de Poisson :\n",
    "\n",
    "1. La variable d'intérêt doit être une variable de comptage, à valeurs entières positives.  \n",
    "\n",
    "2. La variable d'intérêt doit suivre une loi de Poisson :  \n",
    "$Y | X ∼ Poisson(\\lambda)$, où $\\lambda \\in \\mathbb{R+*}$ , en particulier, la moyenne de Y doit être égale à sa variance.\n",
    "\n",
    "3. $log(\\lambda)$ est une fonction linéaire de $X_1, X_2, ... , X_P$  \n",
    "\n",
    "<br>\n",
    "- A posteriori :\n",
    "\n",
    "4. $Cov(X_i, ε_j) = 0$, pour tout $i, j \\in N \\times N, i \\neq j$ (exogénéité) et pour chaque variable explicative $X_p$\n",
    "\n",
    "5. L'erreur suit une loi normale centrée i.e. $E(ε_i) = 0$\n",
    "\n",
    "6. L’erreur est de variance constante (homoscédasticité)\n",
    "i.e $Var(ε_i) = \\sigma^2$, une constante\n",
    "\n",
    "7. Les erreurs sont indépendantes (absence d'autocorrélation)\n",
    "i.e. $Cov(εi, εj) = 0$, pour tout $i, j \\in N \\times N, i \\neq j$\n",
    "\n",
    "8. Absence de colinéarité entre les variables explicatives,\n",
    "i.e. $X^t * X$ est régulière, $det(X^t * X) \\neq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reg[var_categoriques_reg + var_numeriques_reg]\n",
    "Y = df[\"NumStorePurchases\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèses 1 : nature de la variable d'intérêt\n",
    "La variable `NumberStorePurchases` désigne le nombre d'achat effectués en magasin. C'est bien une variable de comptage dont les valeurs sont entières et positives."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 2 : loi de Poisson ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"NumStorePurchases\"], density=True)\n",
    "\n",
    "# creating a numpy array for x-axis\n",
    "x = np.arange(0, 20, 1)\n",
    "\n",
    "# poisson distribution data for y-axis\n",
    "y = poisson.pmf(x, mu=5)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_statistic, p_value = kstest(df[\"NumStorePurchases\"], \"poisson\", args=(5, 0))\n",
    "print(ks_statistic, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Variance de Y :\",\n",
    "    df[\"NumStorePurchases\"].var(),\n",
    "    \": Espérance de Y:\",\n",
    "    df[\"NumStorePurchases\"].mean(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"La variance et l'espérance de Y ne sont pas égales. Cela peut dégrader la qualité de notre régression.\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 3 : lien linéaire entre $log(Y)$ et $X_1, ..., X_P$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_index = Y[Y == 0].index\n",
    "# on élimine les 0 pour appliquer le log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    pd.concat(\n",
    "        [X.drop(index=null_index, axis=0), np.log(Y.drop(index=null_index, axis=0))],\n",
    "        axis=1,\n",
    "    ),\n",
    "    x_vars=var_numeriques_reg[0:4],\n",
    "    y_vars=\"NumStorePurchases\",\n",
    ")\n",
    "sns.pairplot(\n",
    "    pd.concat(\n",
    "        [X.drop(index=null_index, axis=0), np.log(Y.drop(index=null_index, axis=0))],\n",
    "        axis=1,\n",
    "    ),\n",
    "    x_vars=var_numeriques_reg[4:8],\n",
    "    y_vars=\"NumStorePurchases\",\n",
    ")\n",
    "sns.pairplot(\n",
    "    pd.concat(\n",
    "        [X.drop(index=null_index, axis=0), np.log(Y.drop(index=null_index, axis=0))],\n",
    "        axis=1,\n",
    "    ),\n",
    "    x_vars=var_numeriques_reg[8:12],\n",
    "    y_vars=\"NumStorePurchases\",\n",
    ")\n",
    "sns.pairplot(\n",
    "    pd.concat(\n",
    "        [X.drop(index=null_index, axis=0), np.log(Y.drop(index=null_index, axis=0))],\n",
    "        axis=1,\n",
    "    ),\n",
    "    x_vars=var_numeriques_reg[12:],\n",
    "    y_vars=\"NumStorePurchases\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X[var_numeriques_reg].columns:\n",
    "    corr = pearsonr(\n",
    "        X.drop(index=null_index, axis=0)[x], np.log(Y.drop(index=null_index, axis=0))\n",
    "    )\n",
    "    print(\n",
    "        \"Le coefficient de corrélation linéaire entre log(Y) et\",\n",
    "        x,\n",
    "        \"vaut\",\n",
    "        corr[0],\n",
    "        \"la pvalue vaut\",\n",
    "        corr[1],\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe bien un lien linéaire entre $log(Y)$ et les variables explicatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réalisation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PoissonRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_poisson = PoissonRegressor().fit(X_train, Y_train)\n",
    "Y_train_hat = model_poisson.predict(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, notre fonction de lien est $f(\\lambda) = log(\\lambda)$  \n",
    "Elle est utilisée par défaut par sci-kit learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_train, Y_train_hat, color=\"black\")\n",
    "plt.title(\"Valeurs prédites contres vraies valeurs (donnéees d'entraînement)\")\n",
    "abline(1, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 4 : exogénéité sur les variables numériques\n",
    "(Cela n'a pas de sens de tester l'exogénéité sur les variables catégoriques car la notion de corrélation ne fonctionne pas avec celles-ci.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = Y_train - Y_train_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X_train[var_numeriques_reg].columns:\n",
    "    corr = pearsonr(X_train[x], residuals)\n",
    "    print(\n",
    "        \"Le coefficient de corrélation entre\",\n",
    "        x,\n",
    "        \"et les residus vaut\",\n",
    "        corr[0],\n",
    "        \"et la pvalue associée vaut\",\n",
    "        corr[1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il n'y a pas d'endogénéité."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 5 : l'erreur suit une loi normale centrée i.e. $E(ε_i) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(residuals, density=True)\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "plt.plot(x, stats.norm.pdf(x, 0, 1))\n",
    "plt.title(\"Histogramme des résidus en superposition avec la densité de la loi normale\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La moyenne des résidus vaut\", statistics.mean(residuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(residuals, line=\"45\")\n",
    "print(\"On constate sur le qqplot que les points ne suivent pas la droite x = y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Un test de shapiro, pour tester l'hypothèse de normalité, nous donne une pvalue de\",\n",
    "    stats.shapiro(residuals)[1],\n",
    "    \". On rejette l'hypothèse nulle et on conclut que les résidus ne suivent pas une loi normale.\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 6 : homoscédasticité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(residuals, \"bo\")\n",
    "plt.title(\"Nuage de points des résidus\")\n",
    "abline(0, 6.5)\n",
    "abline(0, -6.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = sm.GLM(Y_train, sm.add_constant(X_train), family=sm.families.Poisson()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_names = [\"F statistic\", \"p-value\"]\n",
    "gq_test = sms.het_goldfeldquandt(fit.resid_response, fit.model.exog)\n",
    "lzip(res_names, gq_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La pvalue du test de Goldfeld-Quandt est supérieure à 0.05 dont on conserve l'hypothèse d'homoscédasticité au seuil de 5% (et même au delà)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 7 : absence d'autocorrélation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.tsa.plot_acf(residuals)\n",
    "print(\"On remarque une absence d'autocorrélation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduction du nombre de variables explicatives par le critère AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avant l'AIC, le nombre de variables explicatives est :\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    if (\n",
    "        stepwise(X_train, Y_train, regressor=\"GLMpoisson\")\n",
    "        == \"L'algorithme stepwise est terminé.\"\n",
    "    ):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise(X_train, Y_train, regressor=\"GLMpoisson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Après l'AIC, le nombre de variables explicatives est :\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on refait les modèles après sélection de variables\n",
    "\n",
    "X_test = X_test[\n",
    "    X_train.columns\n",
    "]  # homogénéisation des données de test et d'entraînement\n",
    "\n",
    "fit = sm.GLM(Y_train, sm.add_constant(X_train), family=sm.families.Poisson()).fit()\n",
    "\n",
    "model_poisson = PoissonRegressor().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance de Cook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence = fit.get_influence()\n",
    "cooks = influence.cooks_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train.index, cooks[0])\n",
    "plt.xlabel(\"Index des individus\")\n",
    "plt.ylabel(\"Distances de Cook\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooks_indexes = [i for i, x in enumerate(cooks[0] > 0.010) if x]\n",
    "# détermine l'index dans la liste des individus dont la distance de cook dépasse 0.010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cooks_indexes:\n",
    "    print(X_train.iloc[[i]].index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: supprimer les individus aux distances trop grandes\n",
    "# Remarque : statsmodels ne fait pas d'influence plot pour les modeles GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualité d'ajustement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Le R² du modèle vaut {model_poisson.score(X_train, Y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_hat = model_poisson.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE = {mean_squared_error(Y_train, Y_train_hat)}\")\n",
    "print(f\"RMSE = {mean_squared_error(Y_train, Y_train_hat, squared=False)}\")\n",
    "print(f\"MAE = {mean_absolute_error(Y_train, Y_train_hat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualité de prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"R² du modèle sur les données d'entraînement = {model_poisson.score(X_train, Y_train)}\"\n",
    ")\n",
    "print(f\"R² du modèle sur les données de test = {model_poisson.score(X_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_hat = model_poisson.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_test, Y_test_hat, color=\"black\")\n",
    "abline(1, 0)\n",
    "plt.title(\"Valeurs prédites contre les vraies valeurs : données de test du modèle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE = {mean_squared_error(Y_test, Y_test_hat)}\")\n",
    "print(f\"RMSE = {mean_squared_error(Y_test, Y_test_hat, squared=False)}\")\n",
    "print(f\"MAE = {mean_absolute_error(Y_test, Y_test_hat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Constante : \", model_poisson.intercept_)\n",
    "for i in range(len(model_poisson.coef_)):\n",
    "    print(f\" beta_{i+1} = {model_poisson.coef_[i]} \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression polynomiale\n",
    "\n",
    "### Création des données\n",
    "\n",
    "On conserve les mêmes variables explicatives et la même variable d'intérêt, mais en séparant les variables catégoriques des variables numériques qu'on transformera en monômes.  \n",
    "Ceci car cela n'aurait pas de sens d'élever des variables catégoriques au carré et afin d'éviter l'explosion combinatoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ref = df_reg[var_numeriques_reg + var_categoriques_reg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numeriques = df_reg[var_numeriques_reg]\n",
    "X_categoriques = df_reg[var_categoriques_reg]\n",
    "Y = df_reg[\"NumStorePurchases\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer les données en matrice de caractéristiques polynomiales\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "X_poly = poly.fit_transform(X_numeriques)\n",
    "# Remarque : pendant la transformation, l'ordre des lignes est conservé, ce qui permettra de remettre les index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trouvé sur stack over flow :\n",
    "# permet de créer un df à partir de l'objet numpy array en sortie de la fonction fit_transform de sklearn\n",
    "\n",
    "target_feature_names = [\n",
    "    \"x\".join([\"{}^{}\".format(pair[0], pair[1]) for pair in tuple if pair[1] != 0])\n",
    "    for tuple in [zip(X.columns, p) for p in poly.powers_]\n",
    "]\n",
    "# détermine les noms de colonnes\n",
    "\n",
    "\n",
    "X_poly = pd.DataFrame(X_poly, columns=target_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# je remets les indexes au df X_poly afin de pouvoir le concaténer avec le df des var categoriques avant la régression\n",
    "X_poly = X_poly.set_index(pd.Index(X_categoriques.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_poly, X_categoriques], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien évidemment, il y a de la redondance dans les variables explicatives et donc de la colinéarité."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réalisation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_poly = LinearRegression().fit(X_train, Y_train)\n",
    "Y_train_hat = model_poly.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_train, Y_train_hat, color=\"black\")\n",
    "plt.title(\"Valeurs prédites contres vraies valeurs (donnéees d'entraînement)\")\n",
    "abline(1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduction du nombre de variables explicatives par le critère AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avant l'AIC, le nombre de variables explicatives est :\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    if (\n",
    "        stepwise(\n",
    "            X_train, list(Y_train), regressor=\"linearOLS\"\n",
    "        )  # il faut transformer Y en liste sinon cela bug, je ne sais pas pourquoi\n",
    "        == \"L'algorithme stepwise est terminé.\"\n",
    "    ):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise(X_train, list(Y_train), regressor=\"linearOLS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Après l'AIC, le nombre de variables explicatives est :\", X_train.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La procédure AIC a permi de purifier notre modèle de 77 variables explicatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on refait les modèles après sélection de variables\n",
    "\n",
    "X_test = X_test[\n",
    "    X_train.columns\n",
    "]  # homogénéisation des données de test et d'entraînement\n",
    "\n",
    "fit = sm.OLS(\n",
    "    list(Y_train), sm.add_constant(X_train)\n",
    ").fit()  # comme il existe deja une constante dans X_train, statsmodels n'en rajoute pas une deuxieme\n",
    "\n",
    "model_poly = LinearRegression().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance de Cook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence = fit.get_influence()\n",
    "cooks = influence.cooks_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train.index, cooks[0])\n",
    "plt.xlabel(\"Index des individus\")\n",
    "plt.ylabel(\"Distances de Cook\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooks_indexes = [i for i, x in enumerate(cooks[0] > 0.02) if x]\n",
    "# détermine l'index dans la liste des individus dont la distance de cook dépasse 0.010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_ind_list = [X_train.iloc[[i]].index[0] for i in cooks_indexes]\n",
    "\n",
    "for ind in extreme_ind_list:\n",
    "    print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_plot(fit)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(extreme_ind_list + [6237, 9058], inplace=True)\n",
    "Y_train.drop(extreme_ind_list + [6237, 9058], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on refait les modèles après suppression des individus extremes\n",
    "\n",
    "X_test = X_test[\n",
    "    X_train.columns\n",
    "]  # homogénéisation des données de test et d'entraînement\n",
    "\n",
    "fit = sm.OLS(\n",
    "    list(Y_train), sm.add_constant(X_train)\n",
    ").fit()  # comme il existe deja une constante dans X_train, statsmodels n'en rajoute pas une deuxieme\n",
    "\n",
    "model_poly = LinearRegression().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence = fit.get_influence()\n",
    "cooks = influence.cooks_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train.index, cooks[0])\n",
    "plt.xlabel(\"Index des individus\")\n",
    "plt.ylabel(\"Distances de Cook\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_plot(fit)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualité d'ajustement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Le R² du modèle vaut {model_poly.score(X_train, Y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_hat = model_poly.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE = {mean_squared_error(Y_train, Y_train_hat)}\")\n",
    "print(f\"RMSE = {mean_squared_error(Y_train, Y_train_hat, squared=False)}\")\n",
    "print(f\"MAE = {mean_absolute_error(Y_train, Y_train_hat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualité de prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"R² du modèle sur les données d'entraînement = {model_poly.score(X_train, Y_train)}\"\n",
    ")\n",
    "print(f\"R² du modèle sur les données de test = {model_poly.score(X_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_hat = model_poly.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_test, Y_test_hat, color=\"black\")\n",
    "abline(1, 0)\n",
    "plt.title(\"Valeurs prédites contre les vraies valeurs : données de test du modèle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE = {mean_squared_error(Y_test, Y_test_hat)}\")\n",
    "print(f\"RMSE = {mean_squared_error(Y_test, Y_test_hat, squared=False)}\")\n",
    "print(f\"MAE = {mean_absolute_error(Y_test, Y_test_hat)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mesure de la qualité d'ajustement à nos données de test est biaisée par des valeurs prédites extrêmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(Y_test_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_hat_new = Y_test_hat[Y_test_hat > 0]\n",
    "Y_test_new = Y_test[Y_test_hat > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_test_new, Y_test_hat_new, color=\"black\")\n",
    "abline(1, 0)\n",
    "plt.title(\"Valeurs prédites contre les vraies valeurs : données de test du modèle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE = {mean_squared_error(Y_test_new, Y_test_hat_new)}\")\n",
    "print(f\"RMSE = {mean_squared_error(Y_test_new, Y_test_hat_new, squared=False)}\")\n",
    "print(f\"MAE = {mean_absolute_error(Y_test_new, Y_test_hat_new)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrigé de ces erreurs, le modèle polynomial est notre meilleur modèle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
