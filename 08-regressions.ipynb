{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des outils / jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import xgboost\n",
    "from scipy import stats\n",
    "from scipy.stats import kstest, pearsonr, poisson\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import LinearRegression, PoissonRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from statsmodels.compat import lzip\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"data/data-cleaned-feature-engineering.csv\",\n",
    "    sep=\",\",\n",
    "    index_col=\"ID\",\n",
    "    parse_dates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transforme = pd.read_csv(\n",
    "    \"data/data-transformed.csv\",\n",
    "    sep=\",\",\n",
    "    index_col=\"ID\",\n",
    "    parse_dates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_numeriques = [\n",
    "    \"Year_Birth\",\n",
    "    \"Income\",\n",
    "    \"Recency\",\n",
    "    \"MntWines\",\n",
    "    \"MntFruits\",\n",
    "    \"MntMeatProducts\",\n",
    "    \"MntFishProducts\",\n",
    "    \"MntSweetProducts\",\n",
    "    \"MntGoldProds\",\n",
    "    \"NumDealsPurchases\",\n",
    "    \"NumWebPurchases\",\n",
    "    \"NumCatalogPurchases\",\n",
    "    \"NumStorePurchases\",\n",
    "    \"NumWebVisitsMonth\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_categoriques = [\n",
    "    \"Education\",\n",
    "    \"Marital_Status\",\n",
    "    \"Kidhome\",\n",
    "    \"Teenhome\",\n",
    "    \"AcceptedCmp1\",\n",
    "    \"AcceptedCmp2\",\n",
    "    \"AcceptedCmp3\",\n",
    "    \"AcceptedCmp4\",\n",
    "    \"AcceptedCmp5\",\n",
    "    \"Response\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions et variables utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_modeles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affiche_score(modele, y_test, y_pred):\n",
    "    \"\"\"Affiche la MSE, RMSE et MAE du modèle.\"\"\"\n",
    "    print(f\"MSE = {mean_squared_error(y_test, y_pred)}\")\n",
    "    print(f\"RMSE = {mean_squared_error(y_test, y_pred, squared=False)}\")\n",
    "    print(f\"MAE = {mean_absolute_error(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajout_score(modele, nom_modele, y_test, y_pred):\n",
    "    \"\"\"Ajoute la MMSE, RMSE et MAE au dataframe score_modeles.\"\"\"\n",
    "    score_modeles.extend(\n",
    "        (\n",
    "            [nom_modele, \"mse\", mean_squared_error(y_test, y_pred)],\n",
    "            [nom_modele, \"rmse\", mean_squared_error(y_test, y_pred, squared=False)],\n",
    "            [nom_modele, \"mae\", mean_absolute_error(y_test, y_pred)],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression linéaire simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle simple : une variable à expliquer $Y$ et une seule variable explicative $X$.\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$$\n",
    "\n",
    "Hypothèses à vérifier pour la régression linéaire simple :\n",
    "\n",
    "1) il existe une corrélation linéaire entre $X$ et $Y$\n",
    "\n",
    "1) la distribution de l’erreur $\\epsilon$ est indépendante de la variable X (exogénéité)\n",
    "\n",
    "2) l'erreur suit une loi normale centrée i.e. $E(ε_i) = 0$\n",
    "\n",
    "3) l’erreur est de variance constante (homoscédasticité)\n",
    "i.e $Var(ε_i) = \\sigma^2$, une constante\n",
    "\n",
    "4) les erreurs sont indépendantes (absence d'autocorrélation)\n",
    "i.e. $Cov(εi, εj) = 0$, pour tout $i, j \\in N \\times N, i \\neq j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df_transforme[\"Income\"])\n",
    "Y = np.array(df_transforme[\"NumStorePurchases\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 1 : corrélation linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Le coefficient de corrélation linéaire entre X et Y vaut\",\n",
    "    pearsonr(X, Y)[0],\n",
    "    \"la pvalue associée vaut\",\n",
    "    pearsonr(X, Y)[1],\n",
    "    \"il existe donc bien une relation linéaire entre X et Y.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le reste des hypothèses à tester requiert d'effectuer la régression linéaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X_train.reshape(-1, 1), Y_train)\n",
    "Y_train_hat = model.predict(X_train.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, Y_train, color=\"black\")\n",
    "plt.plot(X_train, Y_train_hat, color=\"red\")\n",
    "plt.title(\"Régression linéaire du nombre d'achats en magasin en fonction du revenu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.intercept_, model.coef_, model.score(X_train.reshape(-1, 1), Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation de régression :\n",
    "\n",
    "$$y_i = 0.42 + 2.21 * x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 2 : exogénéité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = Y_train - Y_train_hat\n",
    "\n",
    "plt.scatter(X_train, residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Le coefficient de corrélation entre X et les residus vaut\",\n",
    "    pearsonr(X_train, residuals)[0],\n",
    "    \". On a bien indépendance et donc exogénéité.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 3 : l'erreur suit une loi normale centrée i.e. E(ε_i) = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(residuals, density=True)\n",
    "\n",
    "x = np.linspace(-7, 7, 100)\n",
    "plt.plot(x, stats.norm.pdf(x, 0, 1))\n",
    "plt.title(\"Histogramme des résidus en superposition avec la densité de la loi normale\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La moyenne des résidus vaut\", statistics.mean(residuals))\n",
    "print(\"Mais les résidus ne suivent pas une loi normale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(residuals, line=\"45\")\n",
    "print(\"On constate sur le qqplot que les points ne suivent pas la droite x = y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Un test de shapiro, pour tester l'hypothèse de normalité, nous donne une pvalue de\",\n",
    "    stats.shapiro(residuals)[1],\n",
    "    \". On rejette l'hypothèse nulle et on conclut que les résidus ne suivent pas une loi normale.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 4 : homoscédasticité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(slope, intercept):\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, \"-\", color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(residuals, \"bo\")\n",
    "plt.title(\"Nuage de points des résidus\")\n",
    "abline(0, 7)\n",
    "abline(0, -7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = sm.OLS(Y_train, sm.add_constant(X_train)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_names = [\"F statistic\", \"p-value\"]\n",
    "gq_test = sms.het_goldfeldquandt(fit.resid, fit.model.exog)\n",
    "lzip(res_names, gq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_test = sms.het_breuschpagan(fit.resid, fit.model.exog)\n",
    "lzip(res_names, bp_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le test de Goldfeld-Quandt ne nous donne pas d'hétéroscédasticité, contraiement au test de Breusch-Pagan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 5 : absence d'autocorrélation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.tsa.plot_acf(residuals)\n",
    "print(\"On remarque une absence d'autocorrélation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance de Cook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence = fit.get_influence()\n",
    "cooks = influence.cooks_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, cooks[0])\n",
    "plt.xlabel(\"Revenus\")\n",
    "plt.ylabel(\"Distances de Cook\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooks_indexes = [i for i, x in enumerate(cooks[0] > 0.010) if x]\n",
    "print(cooks_indexes)\n",
    "# affiche les indexes des individus dont les distances de cook dépassent une certaine valeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_plot(fit)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualité d'ajustement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Le R² du modèle vaut {model.score(X_train.reshape(-1, 1), Y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE = {mean_squared_error(Y_train, Y_train_hat)}\")\n",
    "print(f\"RMSE = {mean_squared_error(Y_train, Y_train_hat, squared=False)}\")\n",
    "print(f\"MAE = {mean_absolute_error(Y_train, Y_train_hat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualité de prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"R² du modèle sur les données d'entraînement = {model.score(X_train.reshape(-1, 1), Y_train)}\"\n",
    ")\n",
    "print(\n",
    "    f\"R² du modèle sur les données de test = {model.score(X_test.reshape(-1, 1), y_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test, y_test, color=\"black\")\n",
    "plt.plot(X_test, y_pred, color=\"red\")\n",
    "plt.title(\"Nombre d'achats en magasin en fonction du revenu : données test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affiche_score(model, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_modele = \"Régression simple\"\n",
    "ajout_score(model, nom_modele, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression linéaire multiple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle multiple : une variable à expliquer $Y$ et $P$ variables explicatives $X_i$\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 X_i(1) + \\beta_2 X_i(2) + \\beta_3 X_i(3) + ... + \\beta_P X_i(P) + \\epsilon_i$$\n",
    "\n",
    "Hypothèses à vérifier pour la régression linéaire multiple :\n",
    "\n",
    "1) il existe une corrélation linéaire entre $X_i$ et $Y$\n",
    "\n",
    "2) $Cov(X_i, ε_j) = 0$, pour tout $i, j \\in N \\times N, i \\neq j$ (exogénéité) et pour chaque variable explicative $X_p$\n",
    "\n",
    "3) l'erreur suit une loi normale centrée i.e. $E(ε_i) = 0$\n",
    "\n",
    "4) l’erreur est de variance constante (homoscédasticité)\n",
    "i.e $Var(ε_i) = \\sigma^2$, une constante\n",
    "\n",
    "5) les erreurs sont indépendantes (absence d'autocorrélation)\n",
    "i.e. $Cov(εi, εj) = 0$, pour tout $i, j \\in N \\times N, i \\neq j$\n",
    "\n",
    "6) absence de colinéarité entre les variables explicatives,\n",
    "i.e. $X^t * X$ est régulière, $det(X^t * X) \\neq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les besoins de la régression linéaire, nous créons des variables muettes (dummy variables) pour inclure les variables catégoriques `Education` et `Marital_status` à la régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = df_transforme.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = pd.get_dummies(df_reg, columns=[\"Education\", \"Marital_Status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cette régression, on suppose que l'entreprise veut profiler au mieux les clients qui achètent dans le magasin. C'est pourquoi la variable d'intérêt pour la régression sera le `nombre d'achats en magasin`.\n",
    "\n",
    "Nos variables explicatives sont des variables numériques et des variables catégoriques transformées en variables muettes.\n",
    "Nous commençons par un grand nombre de variables explicatives puis nous en éliminerons progressivement pendant la vérifications des hypothèses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_numeriques_reg = [\n",
    "    \"Income\",\n",
    "    \"Year_Birth\",\n",
    "    \"Recency\",\n",
    "    \"NbAcceptedCampaigns\",\n",
    "    \"NumWebPurchases\",\n",
    "    \"NumDealsPurchases\",\n",
    "    \"NumWebVisitsMonth\",\n",
    "    \"NumCatalogPurchases\",\n",
    "    \"MntWines\",\n",
    "    \"MntFruits\",\n",
    "    \"MntMeatProducts\",\n",
    "    \"MntFishProducts\",\n",
    "    \"MntSweetProducts\",\n",
    "    \"MntGoldProds\",\n",
    "]\n",
    "\n",
    "var_categoriques_reg = [\n",
    "    \"NbChildren\",\n",
    "    \"Education_2n Cycle\",\n",
    "    \"Education_Basic\",\n",
    "    \"Education_Graduation\",\n",
    "    \"Education_Master\",\n",
    "    \"Education_PhD\",\n",
    "    \"Marital_Status_Divorced\",\n",
    "    \"Marital_Status_Married\",\n",
    "    \"Marital_Status_Single\",\n",
    "    \"Marital_Status_Together\",\n",
    "    \"Marital_Status_Widow\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reg[var_categoriques_reg + var_numeriques_reg]\n",
    "Y = df_reg[\"NumStorePurchases\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Nous avons\",\n",
    "    X.shape[1],\n",
    "    \"variables explicatives au début de la régression dont\",\n",
    "    X[var_numeriques_reg].shape[1],\n",
    "    \"variables numériques.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 1 : lien linéaire entre Y et les variables explicatives numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_transforme, x_vars=var_numeriques_reg[0:4], y_vars=\"NumStorePurchases\")\n",
    "sns.pairplot(df_transforme, x_vars=var_numeriques_reg[4:8], y_vars=\"NumStorePurchases\")\n",
    "sns.pairplot(df_transforme, x_vars=var_numeriques_reg[8:12], y_vars=\"NumStorePurchases\")\n",
    "sns.pairplot(df_transforme, x_vars=var_numeriques_reg[12:], y_vars=\"NumStorePurchases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X[var_numeriques_reg].columns:\n",
    "    corr = pearsonr(X[x], Y)\n",
    "    print(\n",
    "        \"Le coefficient de corrélation linéaire entre Y et\",\n",
    "        x,\n",
    "        \"vaut\",\n",
    "        corr[0],\n",
    "        \"la pvalue vaut\",\n",
    "        corr[1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: faire un joli affichage avec SNS heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On décide déjà d'écarter la variable `Recency` de la régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X[\"Recency\"]\n",
    "var_numeriques_reg.remove(\"Recency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réalisation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multiple = LinearRegression().fit(X_train, Y_train)\n",
    "Y_train_hat = model_multiple.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_train, Y_train_hat, color=\"black\")\n",
    "plt.title(\"Valeurs prédites contres vraies valeurs (donnéees d'entraînement)\")\n",
    "abline(1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 2 : exogénéité sur les variables numériques\n",
    "(Cela n'a pas de sens de tester l'exogénéité sur les variables catégoriques car la notion de corrélation ne fonctionne pas avec celles-ci.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = Y_train - Y_train_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X_train[var_numeriques_reg].columns:\n",
    "    corr = pearsonr(X_train[x], residuals)\n",
    "    print(\n",
    "        \"Le coefficient de corrélation entre\",\n",
    "        x,\n",
    "        \"et les residus vaut\",\n",
    "        corr[0],\n",
    "        \"et la pvalue associée vaut\",\n",
    "        corr[1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il n'y a pas d'endogénéité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 3 : l'erreur suit une loi normale centrée i.e. $E(ε_i) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(residuals, density=True)\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "plt.plot(x, stats.norm.pdf(x, 0, 1))\n",
    "plt.title(\"Histogramme des résidus en superposition avec la densité de la loi normale\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La moyenne des résidus vaut\", statistics.mean(residuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(residuals, line=\"45\")\n",
    "print(\"On constate sur le qqplot que les points ne suivent pas la droite x = y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Un test de shapiro, pour tester l'hypothèse de normalité, nous donne une pvalue de\",\n",
    "    stats.shapiro(residuals)[1],\n",
    "    \". On rejette l'hypothèse nulle et on conclut que les résidus ne suivent pas une loi normale.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 4 : homoscédasticité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(residuals, \"bo\")\n",
    "plt.title(\"Nuage de points des résidus\")\n",
    "abline(0, 6.5)\n",
    "abline(0, -6.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = sm.OLS(Y_train, sm.add_constant(X_train)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_names = [\"F statistic\", \"p-value\"]\n",
    "gq_test = sms.het_goldfeldquandt(fit.resid, fit.model.exog)\n",
    "lzip(res_names, gq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_test = sms.het_breuschpagan(fit.resid, fit.model.exog)\n",
    "lzip(res_names, bp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_test = sms.het_white(fit.resid, fit.model.exog)\n",
    "lzip(res_names, white_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le test de Goldfeld-Quandt, qui vérifie la constance de la variance entre deux échantillons, ne nous donne pas d'hétéroscédasticité, contrairement aux tests de White et de Breusch-Pagan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 5 : absence d'autocorrélation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.tsa.plot_acf(residuals)\n",
    "print(\"On remarque une absence d'autocorrélation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 6 : absence de multicolinéarité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul de la matrice $X^t \\times X$ et de son déterminant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_matrix = X_train.to_numpy()\n",
    "X_train_matrix_transpose = np.matrix.transpose(X_train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det = np.linalg.det(np.matmul(X_train_matrix_transpose, X_train_matrix))\n",
    "det"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance Inflation Factor\n",
    "\n",
    "$$ VIF_i = 1 / ( 1 − R_i^2 ) $$\n",
    "​\n",
    "où $R_i^2 =$ coefficient de détermination non ajusté pour la régression de la ième variable indépendante sur les autres variables restantes\n",
    "​\n",
    "\n",
    "\n",
    "Le facteur d'inflation de la variance est une mesure de l'augmentation de la variance des estimations de paramètres si une variable supplémentaire est ajoutée à la régression linéaire. C'est une mesure de la multicolinéarité de la matrice des variables explicatives.\n",
    "\n",
    "Une recommandation est que si le VIF est supérieur à 10, alors la variable explicative en question est fortement colinéaire avec les autres variables explicatives, et les estimations de paramètres auront de grandes erreurs en raison de cela.\n",
    "\n",
    "*Cf la documentation de statsmodels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = [\n",
    "    variance_inflation_factor(sm.add_constant(X_train).values, i)\n",
    "    for i in range(sm.add_constant(X_train).shape[1])\n",
    "]\n",
    "# Rq: la fonction VIF attend une constante dans le dataframe d'entree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame({\"VIF\": vif[1:]}, index=X_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduction du nombre de variables explicatives par le critère AIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le critère d'information d'Akaike s'écrit comme suit :\n",
    "\n",
    "$$ AIC = 2 \\times k - 2 \\times \\ln(L) $$\n",
    "\n",
    "où k est le nombre de paramètres à estimer du modèle et L est le maximum de la fonction de vraisemblance du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aic(X, Y, regressor=\"\"):  # entrer X le dataframe des variables explicatives,\n",
    "    # Y la variable expliquée, et le regresseur utilisé\n",
    "    aic_list = []\n",
    "\n",
    "    for col in X.columns:\n",
    "        X_temp = X.drop(col, axis=1)\n",
    "\n",
    "        if regressor == \"linearOLS\":\n",
    "            model_temp = sm.OLS(Y, sm.add_constant(X_temp)).fit()\n",
    "        elif regressor == \"GLMpoisson\":\n",
    "            model_temp = sm.GLM(\n",
    "                Y, sm.add_constant(X_temp), family=sm.families.Poisson()\n",
    "            ).fit()\n",
    "\n",
    "        else:\n",
    "            return \"Entrer un régresseur valide\"\n",
    "\n",
    "        aic_list.append([col, model_temp.aic])\n",
    "\n",
    "    return aic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise(X, Y, regressor=\"\"):  # entrer X le dataframe des variables explicatives,\n",
    "    # Y la variable expliquée, et le regresseur utilisé\n",
    "\n",
    "    if regressor == \"linearOLS\":\n",
    "        current_aic = (\n",
    "            sm.OLS(Y, sm.add_constant(X)).fit().aic\n",
    "        )  # aic avec les variables actuelles\n",
    "    elif regressor == \"GLMpoisson\":\n",
    "        current_aic = (\n",
    "            sm.GLM(Y, sm.add_constant(X), family=sm.families.Poisson()).fit().aic\n",
    "        )\n",
    "    else:\n",
    "        return \"Entrer un régresseur valide\"\n",
    "\n",
    "    aic_list = aic(\n",
    "        X, Y, regressor\n",
    "    )  # liste des aic par supression des variables une a une\n",
    "\n",
    "    my_bool = 0  # pour indiquer si aucune variable n'est a enlever\n",
    "    for i in range(\n",
    "        len(aic_list)\n",
    "    ):  # si l'aic diminue pour une variable supprimee, on enleve cette variable du df X\n",
    "\n",
    "        if aic_list[i][1] < current_aic:\n",
    "            del X[aic_list[i][0]]  # a l'interieur des crochets c'est une string\n",
    "            my_bool = 1\n",
    "\n",
    "    if my_bool == 0:\n",
    "        return \"L'algorithme stepwise est terminé.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avant l'AIC, le nombre de variables explicatives est :\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    if (\n",
    "        stepwise(X_train, Y_train, regressor=\"linearOLS\")\n",
    "        == \"L'algorithme stepwise est terminé.\"\n",
    "    ):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise(X_train, Y_train, regressor=\"linearOLS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Après l'AIC, le nombre de variables explicatives est :\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on refait les modèles après sélection de variables\n",
    "\n",
    "X_test = X_test[\n",
    "    X_train.columns\n",
    "]  # homogénéisation des données de test et d'entraînement\n",
    "\n",
    "fit = sm.OLS(Y_train, sm.add_constant(X_train)).fit()\n",
    "\n",
    "model_multiple = LinearRegression().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance de Cook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence = fit.get_influence()\n",
    "cooks = influence.cooks_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train.index, cooks[0])\n",
    "plt.xlabel(\"Index des individus\")\n",
    "plt.ylabel(\"Distances de Cook\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooks_indexes = [i for i, x in enumerate(cooks[0] > 0.010) if x]\n",
    "# détermine l'index dans la liste des individus dont la distance de cook dépasse 0.010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_ind_list = [X_train.iloc[[i]].index[0] for i in cooks_indexes]\n",
    "\n",
    "for ind in extreme_ind_list:\n",
    "    print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: supprimer les individus aux distances trop grandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_plot(fit)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(extreme_ind_list + [5316], inplace=True)\n",
    "Y_train.drop(extreme_ind_list + [5316], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on refait les modèles après suppression des individus extremes\n",
    "\n",
    "X_test = X_test[\n",
    "    X_train.columns\n",
    "]  # homogénéisation des données de test et d'entraînement\n",
    "\n",
    "fit = sm.OLS(Y_train, sm.add_constant(X_train)).fit()\n",
    "\n",
    "model_multiple = LinearRegression().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les distances de Cook ont été largement réduites par la sélection de variables au critère AIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualité d'ajustement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Le R² du modèle vaut {model_multiple.score(X_train, Y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_hat = model_multiple.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train_lineaire_multiple = mean_squared_error(Y_train, Y_train_hat)\n",
    "rmse_train_lineaire_multiple = mean_squared_error(Y_train, Y_train_hat, squared=False)\n",
    "mae_train_lineaire_multiple = mean_absolute_error(Y_train, Y_train_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE = {mse_train_lineaire_multiple}\")\n",
    "print(f\"RMSE = {rmse_train_lineaire_multiple}\")\n",
    "print(f\"MAE = {mae_train_lineaire_multiple}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE = {mean_squared_error(Y_train, Y_train_hat)}\")\n",
    "print(f\"RMSE = {mean_squared_error(Y_train, Y_train_hat, squared=False)}\")\n",
    "print(f\"MAE = {mean_absolute_error(Y_train, Y_train_hat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualité de prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"R² du modèle sur les données d'entraînement = {model_multiple.score(X_train, Y_train)}\"\n",
    ")\n",
    "print(f\"R² du modèle sur les données de test = {model_multiple.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_multiple.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred, color=\"black\")\n",
    "abline(1, 0)\n",
    "plt.title(\"Valeurs prédites contre les vraies valeurs : données de test du modèle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affiche_score(model, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_modele = \"Régression multiple\"\n",
    "ajout_score(model, nom_modele, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression par modèle linéaire généralisé (GLM) : régression de Poisson\n",
    "\n",
    "- Notons $Y$ notre variable d'intérêt et $X_1, ..., X_P$ nos variables explicatives.\n",
    "- On suppose que Y suit une loi de Poisson de paramètre $\\lambda \\in \\mathbb{R+*}$\n",
    "\n",
    "\n",
    "- On cherche à déterminer les coefficients de l'équation :\n",
    "\n",
    "$\\log(\\mathrm{E}(Y|X)) = \\log( \\lambda ) = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_P X_P $\n",
    "\n",
    "Ce qui revient à déterminer les coefficients de l'équation :\n",
    "\n",
    "$\\lambda = e^{\\alpha + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_P X_P}$\n",
    "\n",
    "L'objectif de la régression est d'estimer $\\alpha, \\beta_1, ..., \\beta_P$.\n",
    "Une fois ces coefficients estimés, on peut déterminer, pour un nouveau vecteur $X$,\n",
    "$E(Y|X) = e^{\\alpha + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_P X_P}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèses à vérifier : \n",
    "\n",
    "<br>\n",
    "- Avant de choisir la régression de Poisson :\n",
    "\n",
    "1. La variable d'intérêt doit être une variable de comptage, à valeurs entières positives.  \n",
    "\n",
    "2. La variable d'intérêt doit suivre une loi de Poisson :  \n",
    "$Y | X ∼ Poisson(\\lambda)$, où $\\lambda \\in \\mathbb{R+*}$ , en particulier, la moyenne de Y doit être égale à sa variance.\n",
    "\n",
    "3. $log(\\lambda)$ est une fonction linéaire de $X_1, X_2, ... , X_P$  \n",
    "\n",
    "<br>\n",
    "- A posteriori :\n",
    "\n",
    "4. $Cov(X_i, ε_j) = 0$, pour tout $i, j \\in N \\times N, i \\neq j$ (exogénéité) et pour chaque variable explicative $X_p$\n",
    "\n",
    "5. L'erreur suit une loi normale centrée i.e. $E(ε_i) = 0$\n",
    "\n",
    "6. L’erreur est de variance constante (homoscédasticité)\n",
    "i.e $Var(ε_i) = \\sigma^2$, une constante\n",
    "\n",
    "7. Les erreurs sont indépendantes (absence d'autocorrélation)\n",
    "i.e. $Cov(εi, εj) = 0$, pour tout $i, j \\in N \\times N, i \\neq j$\n",
    "\n",
    "8. Absence de colinéarité entre les variables explicatives,\n",
    "i.e. $X^t * X$ est régulière, $det(X^t * X) \\neq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reg[var_categoriques_reg + var_numeriques_reg]\n",
    "Y = df[\"NumStorePurchases\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèses 1 : nature de la variable d'intérêt\n",
    "La variable `NumberStorePurchases` désigne le nombre d'achat effectués en magasin. C'est bien une variable de comptage dont les valeurs sont entières et positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 2 : loi de Poisson ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"NumStorePurchases\"], density=True)\n",
    "\n",
    "# creating a numpy array for x-axis\n",
    "x = np.arange(0, 20, 1)\n",
    "\n",
    "# poisson distribution data for y-axis\n",
    "y = poisson.pmf(x, mu=5)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_statistic, p_value = kstest(df[\"NumStorePurchases\"], \"poisson\", args=(5, 0))\n",
    "print(ks_statistic, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Variance de Y :\",\n",
    "    df[\"NumStorePurchases\"].var(),\n",
    "    \": Espérance de Y:\",\n",
    "    df[\"NumStorePurchases\"].mean(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"La variance et l'espérance de Y ne sont pas égales. Cela peut dégrader la qualité de notre régression.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 3 : lien linéaire entre $log(Y)$ et $X_1, ..., X_P$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_index = Y[Y == 0].index\n",
    "# on élimine les 0 pour appliquer le log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    pd.concat(\n",
    "        [X.drop(index=null_index, axis=0), np.log(Y.drop(index=null_index, axis=0))],\n",
    "        axis=1,\n",
    "    ),\n",
    "    x_vars=var_numeriques_reg[0:4],\n",
    "    y_vars=\"NumStorePurchases\",\n",
    ")\n",
    "sns.pairplot(\n",
    "    pd.concat(\n",
    "        [X.drop(index=null_index, axis=0), np.log(Y.drop(index=null_index, axis=0))],\n",
    "        axis=1,\n",
    "    ),\n",
    "    x_vars=var_numeriques_reg[4:8],\n",
    "    y_vars=\"NumStorePurchases\",\n",
    ")\n",
    "sns.pairplot(\n",
    "    pd.concat(\n",
    "        [X.drop(index=null_index, axis=0), np.log(Y.drop(index=null_index, axis=0))],\n",
    "        axis=1,\n",
    "    ),\n",
    "    x_vars=var_numeriques_reg[8:12],\n",
    "    y_vars=\"NumStorePurchases\",\n",
    ")\n",
    "sns.pairplot(\n",
    "    pd.concat(\n",
    "        [X.drop(index=null_index, axis=0), np.log(Y.drop(index=null_index, axis=0))],\n",
    "        axis=1,\n",
    "    ),\n",
    "    x_vars=var_numeriques_reg[12:],\n",
    "    y_vars=\"NumStorePurchases\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X[var_numeriques_reg].columns:\n",
    "    corr = pearsonr(\n",
    "        X.drop(index=null_index, axis=0)[x], np.log(Y.drop(index=null_index, axis=0))\n",
    "    )\n",
    "    print(\n",
    "        \"Le coefficient de corrélation linéaire entre log(Y) et\",\n",
    "        x,\n",
    "        \"vaut\",\n",
    "        corr[0],\n",
    "        \"la pvalue vaut\",\n",
    "        corr[1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe bien un lien linéaire entre $log(Y)$ et les variables explicatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réalisation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_poisson = PoissonRegressor().fit(X_train, Y_train)\n",
    "Y_train_hat = model_poisson.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, notre fonction de lien est $f(\\lambda) = log(\\lambda)$  \n",
    "Elle est utilisée par défaut par sci-kit learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_train, Y_train_hat, color=\"black\")\n",
    "plt.title(\"Valeurs prédites contres vraies valeurs (donnéees d'entraînement)\")\n",
    "abline(1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 4 : exogénéité sur les variables numériques\n",
    "(Cela n'a pas de sens de tester l'exogénéité sur les variables catégoriques car la notion de corrélation ne fonctionne pas avec celles-ci.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = Y_train - Y_train_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X_train[var_numeriques_reg].columns:\n",
    "    corr = pearsonr(X_train[x], residuals)\n",
    "    print(\n",
    "        \"Le coefficient de corrélation entre\",\n",
    "        x,\n",
    "        \"et les residus vaut\",\n",
    "        corr[0],\n",
    "        \"et la pvalue associée vaut\",\n",
    "        corr[1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il n'y a pas d'endogénéité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 5 : l'erreur suit une loi normale centrée i.e. $E(ε_i) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(residuals, density=True)\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "plt.plot(x, stats.norm.pdf(x, 0, 1))\n",
    "plt.title(\"Histogramme des résidus en superposition avec la densité de la loi normale\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La moyenne des résidus vaut\", statistics.mean(residuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(residuals, line=\"45\")\n",
    "print(\"On constate sur le qqplot que les points ne suivent pas la droite x = y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Un test de shapiro, pour tester l'hypothèse de normalité, nous donne une pvalue de\",\n",
    "    stats.shapiro(residuals)[1],\n",
    "    \". On rejette l'hypothèse nulle et on conclut que les résidus ne suivent pas une loi normale.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 6 : homoscédasticité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(residuals, \"bo\")\n",
    "plt.title(\"Nuage de points des résidus\")\n",
    "abline(0, 6.5)\n",
    "abline(0, -6.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = sm.GLM(Y_train, sm.add_constant(X_train), family=sm.families.Poisson()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_names = [\"F statistic\", \"p-value\"]\n",
    "gq_test = sms.het_goldfeldquandt(fit.resid_response, fit.model.exog)\n",
    "lzip(res_names, gq_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La pvalue du test de Goldfeld-Quandt est supérieure à 0.05 dont on conserve l'hypothèse d'homoscédasticité au seuil de 5% (et même au delà)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 7 : absence d'autocorrélation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.tsa.plot_acf(residuals)\n",
    "print(\"On remarque une absence d'autocorrélation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothèse 8 : absence de multicolinéarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = [\n",
    "    variance_inflation_factor(sm.add_constant(X_train).values, i)\n",
    "    for i in range(sm.add_constant(X_train).shape[1])\n",
    "]\n",
    "# Rq: la fonction VIF attend une constante dans le dataframe d'entree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame({\"VIF\": vif[1:]}, index=X_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduction du nombre de variables explicatives par le critère AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avant l'AIC, le nombre de variables explicatives est :\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    if (\n",
    "        stepwise(X_train, Y_train, regressor=\"GLMpoisson\")\n",
    "        == \"L'algorithme stepwise est terminé.\"\n",
    "    ):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise(X_train, Y_train, regressor=\"GLMpoisson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Après l'AIC, le nombre de variables explicatives est :\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on refait les modèles après sélection de variables\n",
    "\n",
    "X_test = X_test[\n",
    "    X_train.columns\n",
    "]  # homogénéisation des données de test et d'entraînement\n",
    "\n",
    "fit = sm.GLM(Y_train, sm.add_constant(X_train), family=sm.families.Poisson()).fit()\n",
    "\n",
    "model_poisson = PoissonRegressor().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance de Cook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence = fit.get_influence()\n",
    "cooks = influence.cooks_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train.index, cooks[0])\n",
    "plt.xlabel(\"Index des individus\")\n",
    "plt.ylabel(\"Distances de Cook\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooks_indexes = [i for i, x in enumerate(cooks[0] > 0.010) if x]\n",
    "# détermine l'index dans la liste des individus dont la distance de cook dépasse 0.010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_ind_list = [X_train.iloc[[i]].index[0] for i in cooks_indexes]\n",
    "\n",
    "for ind in extreme_ind_list:\n",
    "    print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(extreme_ind_list, inplace=True)\n",
    "Y_train.drop(extreme_ind_list, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on refait les modèles après suppression des individus extremes\n",
    "\n",
    "X_test = X_test[\n",
    "    X_train.columns\n",
    "]  # homogénéisation des données de test et d'entraînement\n",
    "\n",
    "fit = sm.GLM(Y_train, sm.add_constant(X_train), family=sm.families.Poisson()).fit()\n",
    "\n",
    "model_poisson = PoissonRegressor().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remarque : statsmodels ne fait pas d'influence plot pour les modeles GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualité d'ajustement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Le R² du modèle vaut {model_poisson.score(X_train, Y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_hat = model_poisson.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train_poisson = mean_squared_error(Y_train, Y_train_hat)\n",
    "rmse_train_poisson = mean_squared_error(Y_train, Y_train_hat, squared=False)\n",
    "mae_train_poisson = mean_absolute_error(Y_train, Y_train_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE = {mse_train_poisson}\")\n",
    "print(f\"RMSE = {rmse_train_poisson}\")\n",
    "print(f\"MAE = {mae_train_poisson}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualité de prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"R² du modèle sur les données d'entraînement = {model_poisson.score(X_train, Y_train)}\"\n",
    ")\n",
    "print(f\"R² du modèle sur les données de test = {model_poisson.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_poisson.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred, color=\"black\")\n",
    "abline(1, 0)\n",
    "plt.title(\"Valeurs prédites contre les vraies valeurs : données de test du modèle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affiche_score(model, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_modele = \"GLM Poisson\"\n",
    "ajout_score(model, nom_modele, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Constante : \", model_poisson.intercept_)\n",
    "for i in range(len(model_poisson.coef_)):\n",
    "    print(f\" beta_{i + 1} = {model_poisson.coef_[i]} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression polynomiale\n",
    "\n",
    "### Création des données\n",
    "\n",
    "On conserve les mêmes variables explicatives et la même variable d'intérêt, mais en séparant les variables catégoriques des variables numériques qu'on transformera en monômes.  \n",
    "Ceci car cela n'aurait pas de sens d'élever des variables catégoriques au carré et afin d'éviter l'explosion combinatoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ref = df_reg[var_numeriques_reg + var_categoriques_reg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numeriques = df_reg[var_numeriques_reg]\n",
    "X_categoriques = df_reg[var_categoriques_reg]\n",
    "Y = df_reg[\"NumStorePurchases\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer les données en matrice de caractéristiques polynomiales\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "X_poly = poly.fit_transform(X_numeriques)\n",
    "# Remarque : pendant la transformation, l'ordre des lignes est conservé, ce qui permettra de remettre les index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trouvé sur stack over flow :\n",
    "# permet de créer un df à partir de l'objet numpy array en sortie de la fonction fit_transform de sklearn\n",
    "\n",
    "target_feature_names = [\n",
    "    \"x\".join([\"{}^{}\".format(pair[0], pair[1]) for pair in tuple if pair[1] != 0])\n",
    "    for tuple in [zip(X.columns, p) for p in poly.powers_]\n",
    "]\n",
    "# détermine les noms de colonnes\n",
    "\n",
    "\n",
    "X_poly = pd.DataFrame(X_poly, columns=target_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# je remets les indexes au df X_poly afin de pouvoir le concaténer avec le df des var categoriques avant la régression\n",
    "X_poly = X_poly.set_index(pd.Index(X_categoriques.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_poly, X_categoriques], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien évidemment, il y a de la redondance dans les variables explicatives et donc de la colinéarité structurelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réalisation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_poly = LinearRegression().fit(X_train, Y_train)\n",
    "Y_train_hat = model_poly.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_train, Y_train_hat, color=\"black\")\n",
    "plt.title(\"Valeurs prédites contres vraies valeurs (donnéees d'entraînement)\")\n",
    "abline(1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduction du nombre de variables explicatives par le critère AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avant l'AIC, le nombre de variables explicatives est :\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    if (\n",
    "        stepwise(\n",
    "            X_train, list(Y_train), regressor=\"linearOLS\"\n",
    "        )  # il faut transformer Y en liste sinon cela bug, je ne sais pas pourquoi\n",
    "        == \"L'algorithme stepwise est terminé.\"\n",
    "    ):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise(X_train, list(Y_train), regressor=\"linearOLS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Après l'AIC, le nombre de variables explicatives est :\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La procédure AIC a permi de purifier notre modèle de 77 variables explicatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on refait les modèles après sélection de variables\n",
    "\n",
    "X_test = X_test[\n",
    "    X_train.columns\n",
    "]  # homogénéisation des données de test et d'entraînement\n",
    "\n",
    "fit = sm.OLS(\n",
    "    list(Y_train), sm.add_constant(X_train)\n",
    ").fit()  # comme il existe deja une constante dans X_train, statsmodels n'en rajoute pas une deuxieme\n",
    "\n",
    "model_poly = LinearRegression().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance de Cook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence = fit.get_influence()\n",
    "cooks = influence.cooks_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train.index, cooks[0])\n",
    "plt.xlabel(\"Index des individus\")\n",
    "plt.ylabel(\"Distances de Cook\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooks_indexes = [i for i, x in enumerate(cooks[0] > 0.02) if x]\n",
    "# détermine l'index dans la liste des individus dont la distance de cook dépasse 0.010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_ind_list = [X_train.iloc[[i]].index[0] for i in cooks_indexes]\n",
    "\n",
    "for ind in extreme_ind_list:\n",
    "    print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_plot(fit)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(extreme_ind_list + [6237, 9058], inplace=True)\n",
    "Y_train.drop(extreme_ind_list + [6237, 9058], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on refait les modèles après suppression des individus extremes\n",
    "\n",
    "X_test = X_test[\n",
    "    X_train.columns\n",
    "]  # homogénéisation des données de test et d'entraînement\n",
    "\n",
    "fit = sm.OLS(\n",
    "    list(Y_train), sm.add_constant(X_train)\n",
    ").fit()  # comme il existe deja une constante dans X_train, statsmodels n'en rajoute pas une deuxieme\n",
    "\n",
    "model_poly = LinearRegression().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence = fit.get_influence()\n",
    "cooks = influence.cooks_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train.index, cooks[0])\n",
    "plt.xlabel(\"Index des individus\")\n",
    "plt.ylabel(\"Distances de Cook\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_plot(fit)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualité d'ajustement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Le R² du modèle vaut {model_poly.score(X_train, Y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_hat = model_poly.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train_poly = mean_squared_error(Y_train, Y_train_hat)\n",
    "rmse_train_poly = mean_squared_error(Y_train, Y_train_hat, squared=False)\n",
    "mae_train_poly = mean_absolute_error(Y_train, Y_train_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE = {mse_train_poly}\")\n",
    "print(f\"RMSE = {rmse_train_poly}\")\n",
    "print(f\"MAE = {mae_train_poly}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualité de prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"R² du modèle sur les données d'entraînement = {model_poly.score(X_train, Y_train)}\"\n",
    ")\n",
    "print(f\"R² du modèle sur les données de test = {model_poly.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_poly.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred, color=\"black\")\n",
    "abline(1, 0)\n",
    "plt.title(\"Valeurs prédites contre les vraies valeurs : données de test du modèle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE = {mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"RMSE = {mean_squared_error(y_test, y_pred, squared=False)}\")\n",
    "print(f\"MAE = {mean_absolute_error(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mesure de la qualité d'ajustement à nos données de test est biaisée par des valeurs prédites extrêmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_new = y_pred[y_pred > 0]\n",
    "y_test_new = y_test[y_pred > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test_new, y_pred_new, color=\"black\")\n",
    "abline(1, 0)\n",
    "plt.title(\"Valeurs prédites contre les vraies valeurs : données de test du modèle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affiche_score(model, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_modele = \"Régression polynomiale\"\n",
    "ajout_score(model, nom_modele, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrigé de ces erreurs, le modèle polynomial est notre meilleur modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Constante : \", model_poly.intercept_)\n",
    "for i in range(len(model_poly.coef_)):\n",
    "    print(f\" beta_{i + 1} = {model_poly.coef_[i]} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression PLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous avons de la multi-colinéarité dans notre modèle, nous allons essayer une méthode de régression robuste face à ce phénomène : la régression PLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df_transforme[var_numeriques].drop(columns=[\"NumStorePurchases\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[[\"NumStorePurchases\"]].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_mae_pls = []\n",
    "for n in range(1, 14):\n",
    "    pls = PLSRegression(n_components=n)\n",
    "\n",
    "    pls.fit(X_train, y_train)\n",
    "    pls.score(X_train, y_train)\n",
    "    y_pred = pls.predict(X_test)\n",
    "\n",
    "    liste_mae_pls.append(mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"MAE en fonction du nombre de composantes PLS\")\n",
    "plt.plot(range(1, 14), liste_mae_pls)\n",
    "plt.scatter(np.argmin(liste_mae_pls) + 1, np.min(liste_mae_pls), label=\"minimum\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmin(liste_mae_pls) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls = PLSRegression(n_components=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pls.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affiche_score(model, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_modele = \"Régression PLS\"\n",
    "ajout_score(model, nom_modele, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df_transforme.drop(columns=[\"NumStorePurchases\", \"Dt_Customer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[[\"NumStorePurchases\"]].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_xgb = xgboost.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    n_jobs=4,\n",
    "    eval_metric=\"mae\",\n",
    "    early_stopping_rounds=20,\n",
    "    random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_xgb.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tuned_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affiche_score(model, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_modele = \"XGBoost\"\n",
    "ajout_score(model, nom_modele, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train).astype(\"float32\")\n",
    "y_train = np.asarray(y_train).astype(\"float32\")\n",
    "X_test = np.asarray(X_test).astype(\"float32\")\n",
    "y_test = np.asarray(y_test).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(4, activation=\"relu\", input_shape=[35]),\n",
    "        layers.Dense(4, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mean_absolute_error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    # validation_data=(X_test, y_test),\n",
    "    validation_split=0.2,\n",
    "    batch_size=512,\n",
    "    epochs=1000,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=0,  # hide the output because we have so many epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "# Start the plot at epoch 5\n",
    "history_df.loc[5:, [\"loss\", \"val_loss\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affiche_score(model, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_modele = \"Réseau de Neurones\"\n",
    "ajout_score(model, nom_modele, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_modeles_df = pd.DataFrame(score_modeles, columns=[\"Modèle\", \"Métrique\", \"Valeur\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_modeles_df.to_csv(\"data/results/regressions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
